{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Video Translation - GPU Accelerated Processing\n",
                "\n",
                "This notebook provides GPU-accelerated processing for:\n",
                "- **Whisper Speech-to-Text** (much faster on GPU)\n",
                "- **LLM Translation** (much faster on GPU)\n",
                "\n",
                "## Instructions\n",
                "1. Enable GPU: `Runtime â†’ Change runtime type â†’ Hardware accelerator: GPU`\n",
                "2. Run cells sequentially\n",
                "3. Upload your audio file when prompted\n",
                "4. Download results at the end"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Check GPU Availability"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(\"âœ“ GPU is available\")\n",
                "    print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
                "    print(f\"  CUDA Version: {torch.version.cuda}\")\n",
                "else:\n",
                "    print(\"âœ— No GPU available - using CPU\")\n",
                "    print(\"\\nTo enable GPU:\")\n",
                "    print(\"  Runtime â†’ Change runtime type â†’ Hardware accelerator: GPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip install -q openai-whisper transformers accelerate bitsandbytes\n",
                "print(\"âœ“ Dependencies installed successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip install -q flask flask-cors pyngrok openai-whisper transformers accelerate bitsandbytes\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Whisper Speech-to-Text Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def whisper_transcribe_gpu(audio_path, output_json, output_txt, \n",
                "                          model_size=\"medium\", language=None):\n",
                "    \"\"\"\n",
                "    Transcribe audio using Whisper on GPU\n",
                "    \n",
                "    Args:\n",
                "        audio_path: Path to audio file\n",
                "        output_json: Path to save JSON output\n",
                "        output_txt: Path to save text output\n",
                "        model_size: Whisper model size (tiny, base, small, medium, large)\n",
                "        language: Language code (e.g., 'en', 'es') or None for auto-detect\n",
                "    \n",
                "    Returns:\n",
                "        Transcription result dictionary\n",
                "    \"\"\"\n",
                "    import whisper\n",
                "    import json\n",
                "    \n",
                "    print(f\"Loading Whisper {model_size} on GPU...\")\n",
                "    model = whisper.load_model(model_size, device=\"cuda\")\n",
                "    \n",
                "    print(f\"Transcribing {audio_path}...\")\n",
                "    result = model.transcribe(\n",
                "        audio_path,\n",
                "        language=language,\n",
                "        verbose=True\n",
                "    )\n",
                "    \n",
                "    # Save JSON\n",
                "    with open(output_json, 'w', encoding='utf-8') as f:\n",
                "        json.dump(result, f, indent=2, ensure_ascii=False)\n",
                "    \n",
                "    # Save text\n",
                "    with open(output_txt, 'w', encoding='utf-8') as f:\n",
                "        f.write(result['text'])\n",
                "    \n",
                "    print(f\"\\nâœ“ Transcription saved to {output_json} and {output_txt}\")\n",
                "    print(f\"Detected language: {result.get('language', 'unknown')}\")\n",
                "    print(f\"Number of segments: {len(result.get('segments', []))}\")\n",
                "    \n",
                "    return result\n",
                "\n",
                "print(\"âœ“ Whisper function loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: LLM Translation Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def llm_translate_gpu(input_json, output_json, output_txt,\n",
                "                     target_language, \n",
                "                     model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
                "                     visual_context=None):\n",
                "    \"\"\"\n",
                "    Translate transcription using LLM on GPU\n",
                "    \n",
                "    Args:\n",
                "        input_json: Path to transcription JSON (from Whisper)\n",
                "        output_json: Path to save translated JSON\n",
                "        output_txt: Path to save translated text\n",
                "        target_language: Target language (e.g., 'Spanish', 'French')\n",
                "        model_name: HuggingFace model name\n",
                "        visual_context: Optional visual context string\n",
                "    \n",
                "    Returns:\n",
                "        List of translated segments\n",
                "    \"\"\"\n",
                "    import json\n",
                "    import torch\n",
                "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "    \n",
                "    print(f\"Loading {model_name} on GPU...\")\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        model_name,\n",
                "        torch_dtype=torch.float16,\n",
                "        device_map=\"auto\",\n",
                "        load_in_8bit=True\n",
                "    )\n",
                "    \n",
                "    # Load transcription\n",
                "    with open(input_json, 'r', encoding='utf-8') as f:\n",
                "        transcription = json.load(f)\n",
                "    \n",
                "    segments = transcription.get('segments', [])\n",
                "    print(f\"Translating {len(segments)} segments to {target_language}...\")\n",
                "    \n",
                "    translated_segments = []\n",
                "    \n",
                "    for i, segment in enumerate(segments):\n",
                "        # Build prompt\n",
                "        context_info = f\"\\n\\nVisual Context: {visual_context}\" if visual_context else \"\"\n",
                "        prompt = f\"\"\"[INST] You are a professional translator. Translate the following text to {target_language}.\n",
                "Only provide the translation, nothing else.{context_info}\n",
                "\n",
                "Text to translate: {segment['text']}\n",
                "\n",
                "Translation: [/INST]\"\"\"\n",
                "        \n",
                "        # Generate translation\n",
                "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "        with torch.no_grad():\n",
                "            outputs = model.generate(\n",
                "                **inputs,\n",
                "                max_new_tokens=512,\n",
                "                temperature=0.7,\n",
                "                do_sample=True,\n",
                "                pad_token_id=tokenizer.eos_token_id\n",
                "            )\n",
                "        \n",
                "        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "        translation = full_output.replace(prompt, \"\").strip()\n",
                "        \n",
                "        translated_segments.append({\n",
                "            'start': segment['start'],\n",
                "            'end': segment['end'],\n",
                "            'original': segment['text'],\n",
                "            'translated': translation\n",
                "        })\n",
                "        \n",
                "        if (i + 1) % 5 == 0:\n",
                "            print(f\"  Translated {i + 1}/{len(segments)} segments\")\n",
                "    \n",
                "    # Save JSON\n",
                "    with open(output_json, 'w', encoding='utf-8') as f:\n",
                "        json.dump(translated_segments, f, indent=2, ensure_ascii=False)\n",
                "    \n",
                "    # Save text\n",
                "    full_text = \" \".join([seg['translated'] for seg in translated_segments])\n",
                "    with open(output_txt, 'w', encoding='utf-8') as f:\n",
                "        f.write(full_text)\n",
                "    \n",
                "    print(f\"\\nâœ“ Translation saved to {output_json} and {output_txt}\")\n",
                "    return translated_segments\n",
                "\n",
                "print(\"âœ“ Translation function loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Upload Audio File"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "\n",
                "print(\"Please upload your audio file:\")\n",
                "uploaded = files.upload()\n",
                "audio_file = list(uploaded.keys())[0]\n",
                "print(f\"\\nâœ“ Uploaded: {audio_file}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Run Whisper Transcription"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configure settings\n",
                "WHISPER_MODEL = \"medium\"  # Options: tiny, base, small, medium, large\n",
                "SOURCE_LANGUAGE = None    # None for auto-detect, or 'en', 'es', 'fr', etc.\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"RUNNING WHISPER SPEECH-TO-TEXT\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "transcription_result = whisper_transcribe_gpu(\n",
                "    audio_path=audio_file,\n",
                "    output_json=\"transcription.json\",\n",
                "    output_txt=\"transcription.txt\",\n",
                "    model_size=WHISPER_MODEL,\n",
                "    language=SOURCE_LANGUAGE\n",
                ")\n",
                "\n",
                "print(\"\\nâœ“ Transcription complete!\")\n",
                "print(f\"\\nTranscribed text preview:\")\n",
                "print(transcription_result['text'][:500] + \"...\" if len(transcription_result['text']) > 500 else transcription_result['text'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Run LLM Translation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configure settings\n",
                "TARGET_LANGUAGE = \"Spanish\"  # Change to your target language\n",
                "LLM_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"  # Or use another model\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(f\"RUNNING LLM TRANSLATION TO {TARGET_LANGUAGE.upper()}\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "translation_result = llm_translate_gpu(\n",
                "    input_json=\"transcription.json\",\n",
                "    output_json=\"translation.json\",\n",
                "    output_txt=\"translation.txt\",\n",
                "    target_language=TARGET_LANGUAGE,\n",
                "    model_name=LLM_MODEL\n",
                ")\n",
                "\n",
                "print(\"\\nâœ“ Translation complete!\")\n",
                "print(f\"\\nTranslated text preview:\")\n",
                "preview_text = \" \".join([seg['translated'] for seg in translation_result[:3]])\n",
                "print(preview_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Download Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "\n",
                "print(\"Downloading results...\\n\")\n",
                "\n",
                "files.download(\"transcription.json\")\n",
                "files.download(\"transcription.txt\")\n",
                "files.download(\"translation.json\")\n",
                "files.download(\"translation.txt\")\n",
                "\n",
                "print(\"\\nâœ“ All files downloaded!\")\n",
                "print(\"\\nNext steps:\")\n",
                "print(\"1. Use these files in your local pipeline\")\n",
                "print(\"2. The translation.json contains timing information\")\n",
                "print(\"3. Feed to gTTS for speech synthesis\")\n",
                "print(\"4. Reconstruct video with new audio\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!ngrok config add-authtoken 35QmdBdgUvzBVxpSZRTs4iAEgu9_2w7NRNiXPG4QeANGqmYHQ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies and start Flask server\n",
                "\n",
                "import os\n",
                "import json\n",
                "from flask import Flask, request, jsonify\n",
                "from flask_cors import CORS\n",
                "import whisper\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "from pyngrok import ngrok\n",
                "import tempfile\n",
                "\n",
                "app = Flask(__name__)\n",
                "CORS(app)\n",
                "\n",
                "# Global model cache\n",
                "whisper_model = None\n",
                "llm_model = None\n",
                "llm_tokenizer = None\n",
                "\n",
                "@app.route('/health', methods=['GET'])\n",
                "def health():\n",
                "    '''Health check endpoint'''\n",
                "    gpu_available = torch.cuda.is_available()\n",
                "    gpu_name = torch.cuda.get_device_name(0) if gpu_available else \"No GPU\"\n",
                "    \n",
                "    return jsonify({\n",
                "        'status': 'healthy',\n",
                "        'gpu_available': gpu_available,\n",
                "        'gpu_name': gpu_name,\n",
                "        'whisper_loaded': whisper_model is not None,\n",
                "        'llm_loaded': llm_model is not None\n",
                "    })\n",
                "\n",
                "@app.route('/load_whisper', methods=['POST'])\n",
                "def load_whisper_model():\n",
                "    '''Load Whisper model into memory'''\n",
                "    global whisper_model\n",
                "    \n",
                "    data = request.json\n",
                "    model_size = data.get('model_size', 'medium')\n",
                "    \n",
                "    print(f\"Loading Whisper {model_size}...\")\n",
                "    whisper_model = whisper.load_model(model_size, device=\"cuda\")\n",
                "    \n",
                "    return jsonify({\n",
                "        'status': 'success',\n",
                "        'message': f'Whisper {model_size} loaded on GPU'\n",
                "    })\n",
                "\n",
                "@app.route('/load_llm', methods=['POST'])\n",
                "def load_llm_model():\n",
                "    '''Load LLM model into memory'''\n",
                "    global llm_model, llm_tokenizer\n",
                "    \n",
                "    data = request.json\n",
                "    model_name = data.get('model_name', 'mistralai/Mistral-7B-Instruct-v0.2')\n",
                "    \n",
                "    print(f\"Loading {model_name}...\")\n",
                "    llm_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
                "        model_name,\n",
                "        torch_dtype=torch.float16,\n",
                "        device_map=\"auto\",\n",
                "        load_in_8bit=True\n",
                "    )\n",
                "    \n",
                "    return jsonify({\n",
                "        'status': 'success',\n",
                "        'message': f'{model_name} loaded on GPU'\n",
                "    })\n",
                "\n",
                "@app.route('/whisper/transcribe', methods=['POST'])\n",
                "def transcribe():\n",
                "    '''Transcribe audio using Whisper'''\n",
                "    global whisper_model\n",
                "    \n",
                "    if whisper_model is None:\n",
                "        return jsonify({'error': 'Whisper model not loaded. Call /load_whisper first'}), 400\n",
                "    \n",
                "    # Get audio file\n",
                "    if 'audio' not in request.files:\n",
                "        return jsonify({'error': 'No audio file provided'}), 400\n",
                "    \n",
                "    audio_file = request.files['audio']\n",
                "    language = request.form.get('language', None)\n",
                "    \n",
                "    # Save to temp file\n",
                "    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_audio:\n",
                "        audio_file.save(temp_audio.name)\n",
                "        temp_path = temp_audio.name\n",
                "    \n",
                "    try:\n",
                "        print(f\"Transcribing {temp_path}...\")\n",
                "        result = whisper_model.transcribe(\n",
                "            temp_path,\n",
                "            language=language if language != 'auto' else None,\n",
                "            verbose=False\n",
                "        )\n",
                "        \n",
                "        # Clean up\n",
                "        os.unlink(temp_path)\n",
                "        \n",
                "        return jsonify({\n",
                "            'status': 'success',\n",
                "            'result': result\n",
                "        })\n",
                "        \n",
                "    except Exception as e:\n",
                "        os.unlink(temp_path)\n",
                "        return jsonify({'error': str(e)}), 500\n",
                "\n",
                "@app.route('/llm/translate', methods=['POST'])\n",
                "def translate():\n",
                "    '''Translate text using LLM'''\n",
                "    global llm_model, llm_tokenizer\n",
                "    \n",
                "    if llm_model is None or llm_tokenizer is None:\n",
                "        return jsonify({'error': 'LLM model not loaded. Call /load_llm first'}), 400\n",
                "    \n",
                "    data = request.json\n",
                "    segments = data.get('segments', [])\n",
                "    target_language = data.get('target_language', 'Spanish')\n",
                "    visual_context = data.get('visual_context', None)\n",
                "    \n",
                "    print(f\"Translating {len(segments)} segments to {target_language}...\")\n",
                "    \n",
                "    translated_segments = []\n",
                "    \n",
                "    for i, segment in enumerate(segments):\n",
                "        # Build prompt\n",
                "        context_info = f\"\\n\\nVisual Context: {visual_context}\" if visual_context else \"\"\n",
                "        prompt = f\"\"\"[INST] You are a professional translator. Translate the following text to {target_language}.\n",
                "Only provide the translation, nothing else.{context_info}\n",
                "\n",
                "Text to translate: {segment['text']}\n",
                "\n",
                "Translation: [/INST]\"\"\"\n",
                "        \n",
                "        # Generate\n",
                "        inputs = llm_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "        with torch.no_grad():\n",
                "            outputs = llm_model.generate(\n",
                "                **inputs,\n",
                "                max_new_tokens=512,\n",
                "                temperature=0.7,\n",
                "                do_sample=True,\n",
                "                pad_token_id=llm_tokenizer.eos_token_id\n",
                "            )\n",
                "        \n",
                "        full_output = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "        translation = full_output.replace(prompt, \"\").strip()\n",
                "        \n",
                "        translated_segments.append({\n",
                "            'start': segment['start'],\n",
                "            'end': segment['end'],\n",
                "            'original': segment['text'],\n",
                "            'translated': translation\n",
                "        })\n",
                "        \n",
                "        if (i + 1) % 5 == 0:\n",
                "            print(f\"  Translated {i + 1}/{len(segments)}\")\n",
                "    \n",
                "    return jsonify({\n",
                "        'status': 'success',\n",
                "        'translated_segments': translated_segments\n",
                "    })\n",
                "\n",
                "# Start ngrok tunnel\n",
                "print(\"Starting ngrok tunnel...\")\n",
                "public_url = ngrok.connect(5000)\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"ðŸš€ Colab GPU Server is running!\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"Public URL: {public_url}\")\n",
                "print(f\"\\nAdd this to your local .env file:\")\n",
                "print(f\"COLAB_API_URL={public_url}\")\n",
                "print(f\"USE_COLAB_GPU=True\")\n",
                "print(f\"{'='*60}\\n\")\n",
                "\n",
                "# Run Flask\n",
                "app.run(port=5000)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Optional: Quick Test with Sample Text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quick test without uploading files\n",
                "# Uncomment to test the translation function directly\n",
                "\n",
                "# import json\n",
                "# \n",
                "# sample_transcription = {\n",
                "#     \"text\": \"Hello, how are you today?\",\n",
                "#     \"language\": \"en\",\n",
                "#     \"segments\": [\n",
                "#         {\"start\": 0.0, \"end\": 2.0, \"text\": \"Hello, how are you today?\"}\n",
                "#     ]\n",
                "# }\n",
                "# \n",
                "# with open(\"test_transcription.json\", \"w\") as f:\n",
                "#     json.dump(sample_transcription, f)\n",
                "# \n",
                "# test_result = llm_translate_gpu(\n",
                "#     input_json=\"test_transcription.json\",\n",
                "#     output_json=\"test_translation.json\",\n",
                "#     output_txt=\"test_translation.txt\",\n",
                "#     target_language=\"French\"\n",
                "# )\n",
                "# \n",
                "# print(\"Test translation:\", test_result[0]['translated'])"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
