{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Colab GPU Server - API Mode\n",
                "\n",
                "This notebook runs a Flask server that your local FastAPI can call for GPU acceleration.\n",
                "\n",
                "## Setup\n",
                "1. **Enable GPU**: Runtime â†’ Change runtime type â†’ Hardware accelerator: GPU\n",
                "2. **Run the cell below** - It will start a server and give you a public URL\n",
                "3. **Copy the URL** and add it to your local `.env` file\n",
                "4. **Your local API** will now use Colab's GPU automatically!\n",
                "\n",
                "âš ï¸ **Keep this cell running** - Don't stop it while processing videos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "python"
                }
            },
            "outputs": [],
            "source": [
                "!pip install -q flask flask-cors pyngrok openai-whisper transformers accelerate bitsandbytes sentencepiece\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "python"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting ngrok tunnel...\n",
                        "\\n==================================================\n",
                        "ðŸš€ COLAB GPU SERVER READY\n",
                        "==================================================\n",
                        "URL: NgrokTunnel: \"https://untattered-woesome-teri.ngrok-free.dev\" -> \"http://localhost:5000\"\n",
                        "\\nSet in your local .env:\n",
                        "  COLAB_API_URL=NgrokTunnel: \"https://untattered-woesome-teri.ngrok-free.dev\" -> \"http://localhost:5000\"\n",
                        "  USE_COLAB_GPU=True\n",
                        "==================================================\n",
                        "\\nEndpoints:\n",
                        "  POST /load_whisper  - Load Whisper\n",
                        "  POST /load_refiner  - Load Flan-T5\n",
                        "  POST /whisper/transcribe - Transcribe audio\n",
                        "  POST /llm/refine - Fix sentences\n",
                        "==================================================\\n\n",
                        " * Serving Flask app '__main__'\n",
                        " * Debug mode: off\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
                        " * Running on http://127.0.0.1:5000\n",
                        "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading Whisper medium on GPU...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:werkzeug:127.0.0.1 - - [07/Jan/2026 14:34:16] \"POST /load_whisper HTTP/1.1\" 200 -\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Whisper loaded!\n",
                        "Transcribing...\n",
                        "Detected language: English\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4943/4943 [00:06<00:00, 716.00frames/s]\n",
                        "INFO:werkzeug:127.0.0.1 - - [07/Jan/2026 14:34:27] \"POST /whisper/transcribe HTTP/1.1\" 200 -\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading google/flan-t5-large on GPU...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
                        "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
                        "You are not authenticated with the Hugging Face Hub in this notebook.\n",
                        "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
                        "  warnings.warn(\n",
                        "`torch_dtype` is deprecated! Use `dtype` instead!\n",
                        "INFO:werkzeug:127.0.0.1 - - [07/Jan/2026 14:34:57] \"POST /load_refiner HTTP/1.1\" 200 -\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Refiner loaded!\n",
                        "Refining 11 segments on GPU...\n",
                        "  10/11 done\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:werkzeug:127.0.0.1 - - [07/Jan/2026 14:35:07] \"POST /llm/refine HTTP/1.1\" 200 -\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Refinement complete!\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "import os\n",
                "from flask import Flask, request, jsonify\n",
                "from flask_cors import CORS\n",
                "import whisper\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
                "from pyngrok import ngrok\n",
                "import tempfile\n",
                "\n",
                "app = Flask(__name__)\n",
                "CORS(app)\n",
                "\n",
                "# Global model cache\n",
                "whisper_model = None\n",
                "refiner_model = None\n",
                "refiner_tokenizer = None\n",
                "\n",
                "@app.route('/health', methods=['GET'])\n",
                "def health():\n",
                "    '''Health check'''\n",
                "    return jsonify({\n",
                "        'status': 'healthy',\n",
                "        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\",\n",
                "        'whisper_loaded': whisper_model is not None,\n",
                "        'refiner_loaded': refiner_model is not None\n",
                "    })\n",
                "\n",
                "@app.route('/load_whisper', methods=['POST'])\n",
                "def load_whisper():\n",
                "    '''Load Whisper model on GPU'''\n",
                "    global whisper_model\n",
                "    \n",
                "    data = request.json or {}\n",
                "    model_size = data.get('model_size', 'medium')\n",
                "    \n",
                "    print(f\"Loading Whisper {model_size} on GPU...\")\n",
                "    whisper_model = whisper.load_model(model_size, device=\"cuda\")\n",
                "    print(\"âœ… Whisper loaded!\")\n",
                "    \n",
                "    return jsonify({'status': 'success', 'model': model_size})\n",
                "\n",
                "@app.route('/load_refiner', methods=['POST'])\n",
                "def load_refiner():\n",
                "    '''Load LLM refiner (Flan-T5) on GPU'''\n",
                "    global refiner_model, refiner_tokenizer\n",
                "    \n",
                "    data = request.json or {}\n",
                "    model_name = data.get('model_name', 'google/flan-t5-large')\n",
                "    \n",
                "    print(f\"Loading {model_name} on GPU...\")\n",
                "    refiner_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    \n",
                "    if 't5' in model_name.lower() or 'flan' in model_name.lower():\n",
                "        refiner_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
                "            model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
                "        )\n",
                "    else:\n",
                "        refiner_model = AutoModelForCausalLM.from_pretrained(\n",
                "            model_name, torch_dtype=torch.float16, device_map=\"auto\", load_in_8bit=True\n",
                "        )\n",
                "    print(\"âœ… Refiner loaded!\")\n",
                "    \n",
                "    return jsonify({'status': 'success', 'model': model_name})\n",
                "\n",
                "# Alias for compatibility\n",
                "@app.route('/load_llm', methods=['POST'])\n",
                "def load_llm():\n",
                "    return load_refiner()\n",
                "\n",
                "@app.route('/whisper/transcribe', methods=['POST'])\n",
                "def transcribe():\n",
                "    '''Transcribe audio using Whisper on GPU'''\n",
                "    global whisper_model\n",
                "    \n",
                "    if whisper_model is None:\n",
                "        return jsonify({'error': 'Call /load_whisper first'}), 400\n",
                "    \n",
                "    if 'audio' not in request.files:\n",
                "        return jsonify({'error': 'No audio file'}), 400\n",
                "    \n",
                "    audio_file = request.files['audio']\n",
                "    language = request.form.get('language', None)\n",
                "    \n",
                "    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as f:\n",
                "        audio_file.save(f.name)\n",
                "        temp_path = f.name\n",
                "    \n",
                "    try:\n",
                "        print(\"Transcribing...\")\n",
                "        result = whisper_model.transcribe(\n",
                "            temp_path,\n",
                "            language=language if language != 'auto' else None,\n",
                "            verbose=False\n",
                "        )\n",
                "        os.unlink(temp_path)\n",
                "        return jsonify({'status': 'success', 'result': result})\n",
                "    except Exception as e:\n",
                "        os.unlink(temp_path)\n",
                "        return jsonify({'error': str(e)}), 500\n",
                "\n",
                "@app.route('/llm/refine', methods=['POST'])\n",
                "def refine():\n",
                "    '''Refine/fix transcription segments using LLM on GPU'''\n",
                "    global refiner_model, refiner_tokenizer\n",
                "    \n",
                "    if refiner_model is None:\n",
                "        return jsonify({'error': 'Call /load_refiner first'}), 400\n",
                "    \n",
                "    data = request.json\n",
                "    segments = data.get('segments', [])\n",
                "    visual_context = data.get('visual_context', None)\n",
                "    \n",
                "    print(f\"Refining {len(segments)} segments on GPU...\")\n",
                "    refined_segments = []\n",
                "    \n",
                "    for i, seg in enumerate(segments):\n",
                "        text = seg.get('text', '')\n",
                "        \n",
                "        if visual_context:\n",
                "            prompt = f\"Fix grammar and complete sentences. Context: {visual_context}. Text: {text}\"\n",
                "        else:\n",
                "            prompt = f\"Fix grammar and complete sentences: {text}\"\n",
                "        \n",
                "        inputs = refiner_tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True).to(\"cuda\")\n",
                "        with torch.no_grad():\n",
                "            outputs = refiner_model.generate(**inputs, max_new_tokens=256, temperature=0.3, do_sample=True)\n",
                "        \n",
                "        refined = refiner_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "        \n",
                "        refined_segments.append({\n",
                "            'start': seg['start'],\n",
                "            'end': seg['end'],\n",
                "            'original': text,\n",
                "            'refined': refined.strip()\n",
                "        })\n",
                "        \n",
                "        if (i + 1) % 10 == 0:\n",
                "            print(f\"  {i + 1}/{len(segments)} done\")\n",
                "    \n",
                "    print(\"âœ… Refinement complete!\")\n",
                "    return jsonify({'status': 'success', 'refined_segments': refined_segments})\n",
                "\n",
                "# Start server\n",
                "print(\"Starting ngrok tunnel...\")\n",
                "public_url = ngrok.connect(5000)\n",
                "print(f\"\\\\n{'='*50}\")\n",
                "print(f\"ðŸš€ COLAB GPU SERVER READY\")\n",
                "print(f\"{'='*50}\")\n",
                "print(f\"URL: {public_url}\")\n",
                "print(f\"\\\\nSet in your local .env:\")\n",
                "print(f\"  COLAB_API_URL={public_url}\")\n",
                "print(f\"  USE_COLAB_GPU=True\")\n",
                "print(f\"{'='*50}\")\n",
                "print(f\"\\\\nEndpoints:\")\n",
                "print(f\"  POST /load_whisper  - Load Whisper\")\n",
                "print(f\"  POST /load_refiner  - Load Flan-T5\")\n",
                "print(f\"  POST /whisper/transcribe - Transcribe audio\")\n",
                "print(f\"  POST /llm/refine - Fix sentences\")\n",
                "print(f\"{'='*50}\\\\n\")\n",
                "\n",
                "app.run(port=5000)\n",
                "# \"\"\"\n",
                "\n",
                "# print(\"=\"*60)\n",
                "# print(\"COLAB GPU SERVER - Whisper + LLM Refinement ONLY\")\n",
                "# print(\"=\"*60)\n",
                "# print(\"\\\\nThis server handles ONLY:\")\n",
                "# print(\"  1. Whisper transcription (GPU)\")\n",
                "# print(\"  2. LLM sentence refinement (GPU)\")\n",
                "# print(\"\\\\nTranslation, TTS, etc. run locally.\")\n",
                "# print(\"=\"*60)\n",
                "# print(\"\\\\nCopy below into a Colab cell:\")\n",
                "# print(\"=\"*60)\n",
                "# print(CODE)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## What Happens Next\n",
                "\n",
                "Once this cell is running:\n",
                "\n",
                "1. **You'll see a public URL** (e.g., `https://xxxx.ngrok.io`)\n",
                "2. **Copy that URL** and add it to your local `.env` file\n",
                "3. **Set `USE_COLAB_GPU=True`** in your `.env`\n",
                "4. **Restart your local FastAPI server**\n",
                "5. **Upload videos as normal** - they'll use Colab's GPU automatically!\n",
                "\n",
                "The server will:\n",
                "- Auto-load Whisper model on first use\n",
                "- Auto-load LLM model on first use\n",
                "- Keep models in memory for fast processing\n",
                "- Process requests from your local machine\n",
                "\n",
                "âš ï¸ Keep this notebook open and the cell running while processing videos!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "python"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
                    ]
                }
            ],
            "source": [
                "!ngrok config add-authtoken 35QmdBdgUvzBVxpSZRTs4iAEgu9_2w7NRNiXPG4QeANGqmYHQ"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "",
            "version": ""
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
