{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Colab GPU Server - API Mode\n",
                "\n",
                "This notebook runs a Flask server that your local FastAPI can call for GPU acceleration.\n",
                "\n",
                "## Setup\n",
                "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator: GPU\n",
                "2. **Run the cell below** - It will start a server and give you a public URL\n",
                "3. **Copy the URL** and add it to your local `.env` file\n",
                "4. **Your local API** will now use Colab's GPU automatically!\n",
                "\n",
                "‚ö†Ô∏è **Keep this cell running** - Don't stop it while processing videos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q flask flask-cors pyngrok openai-whisper transformers accelerate bitsandbytes\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies and start Flask server\n",
                "\n",
                "import os\n",
                "import json\n",
                "from flask import Flask, request, jsonify\n",
                "from flask_cors import CORS\n",
                "import whisper\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "from pyngrok import ngrok\n",
                "import tempfile\n",
                "\n",
                "app = Flask(__name__)\n",
                "CORS(app)\n",
                "\n",
                "# Global model cache\n",
                "whisper_model = None\n",
                "llm_model = None\n",
                "llm_tokenizer = None\n",
                "\n",
                "@app.route('/health', methods=['GET'])\n",
                "def health():\n",
                "    '''Health check endpoint'''\n",
                "    gpu_available = torch.cuda.is_available()\n",
                "    gpu_name = torch.cuda.get_device_name(0) if gpu_available else \"No GPU\"\n",
                "    \n",
                "    return jsonify({\n",
                "        'status': 'healthy',\n",
                "        'gpu_available': gpu_available,\n",
                "        'gpu_name': gpu_name,\n",
                "        'whisper_loaded': whisper_model is not None,\n",
                "        'llm_loaded': llm_model is not None\n",
                "    })\n",
                "\n",
                "@app.route('/load_whisper', methods=['POST'])\n",
                "def load_whisper_model():\n",
                "    '''Load Whisper model into memory'''\n",
                "    global whisper_model\n",
                "    \n",
                "    data = request.json\n",
                "    model_size = data.get('model_size', 'medium')\n",
                "    \n",
                "    print(f\"Loading Whisper {model_size}...\")\n",
                "    whisper_model = whisper.load_model(model_size, device=\"cuda\")\n",
                "    \n",
                "    return jsonify({\n",
                "        'status': 'success',\n",
                "        'message': f'Whisper {model_size} loaded on GPU'\n",
                "    })\n",
                "\n",
                "@app.route('/load_llm', methods=['POST'])\n",
                "def load_llm_model():\n",
                "    '''Load LLM model into memory'''\n",
                "    global llm_model, llm_tokenizer\n",
                "    \n",
                "    data = request.json\n",
                "    model_name = data.get('model_name', 'mistralai/Mistral-7B-Instruct-v0.2')\n",
                "    \n",
                "    print(f\"Loading {model_name}...\")\n",
                "    llm_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
                "        model_name,\n",
                "        torch_dtype=torch.float16,\n",
                "        device_map=\"auto\",\n",
                "        load_in_8bit=True\n",
                "    )\n",
                "    \n",
                "    return jsonify({\n",
                "        'status': 'success',\n",
                "        'message': f'{model_name} loaded on GPU'\n",
                "    })\n",
                "\n",
                "@app.route('/whisper/transcribe', methods=['POST'])\n",
                "def transcribe():\n",
                "    '''Transcribe audio using Whisper'''\n",
                "    global whisper_model\n",
                "    \n",
                "    if whisper_model is None:\n",
                "        return jsonify({'error': 'Whisper model not loaded. Call /load_whisper first'}), 400\n",
                "    \n",
                "    # Get audio file\n",
                "    if 'audio' not in request.files:\n",
                "        return jsonify({'error': 'No audio file provided'}), 400\n",
                "    \n",
                "    audio_file = request.files['audio']\n",
                "    language = request.form.get('language', None)\n",
                "    \n",
                "    # Save to temp file\n",
                "    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_audio:\n",
                "        audio_file.save(temp_audio.name)\n",
                "        temp_path = temp_audio.name\n",
                "    \n",
                "    try:\n",
                "        print(f\"Transcribing {temp_path}...\")\n",
                "        result = whisper_model.transcribe(\n",
                "            temp_path,\n",
                "            language=language if language != 'auto' else None,\n",
                "            verbose=False\n",
                "        )\n",
                "        \n",
                "        # Clean up\n",
                "        os.unlink(temp_path)\n",
                "        \n",
                "        return jsonify({\n",
                "            'status': 'success',\n",
                "            'result': result\n",
                "        })\n",
                "        \n",
                "    except Exception as e:\n",
                "        os.unlink(temp_path)\n",
                "        return jsonify({'error': str(e)}), 500\n",
                "\n",
                "@app.route('/llm/translate', methods=['POST'])\n",
                "def translate():\n",
                "    '''Translate text using LLM'''\n",
                "    global llm_model, llm_tokenizer\n",
                "    \n",
                "    if llm_model is None or llm_tokenizer is None:\n",
                "        return jsonify({'error': 'LLM model not loaded. Call /load_llm first'}), 400\n",
                "    \n",
                "    data = request.json\n",
                "    segments = data.get('segments', [])\n",
                "    target_language = data.get('target_language', 'Spanish')\n",
                "    visual_context = data.get('visual_context', None)\n",
                "    \n",
                "    print(f\"Translating {len(segments)} segments to {target_language}...\")\n",
                "    \n",
                "    translated_segments = []\n",
                "    \n",
                "    for i, segment in enumerate(segments):\n",
                "        # Build prompt\n",
                "        context_info = f\"\\n\\nVisual Context: {visual_context}\" if visual_context else \"\"\n",
                "        prompt = f\"\"\"[INST] You are a professional translator. Translate the following text to {target_language}.\n",
                "Only provide the translation, nothing else.{context_info}\n",
                "\n",
                "Text to translate: {segment['text']}\n",
                "\n",
                "Translation: [/INST]\"\"\"\n",
                "        \n",
                "        # Generate\n",
                "        inputs = llm_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "        with torch.no_grad():\n",
                "            outputs = llm_model.generate(\n",
                "                **inputs,\n",
                "                max_new_tokens=512,\n",
                "                temperature=0.7,\n",
                "                do_sample=True,\n",
                "                pad_token_id=llm_tokenizer.eos_token_id\n",
                "            )\n",
                "        \n",
                "        full_output = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "        translation = full_output.replace(prompt, \"\").strip()\n",
                "        \n",
                "        translated_segments.append({\n",
                "            'start': segment['start'],\n",
                "            'end': segment['end'],\n",
                "            'original': segment['text'],\n",
                "            'translated': translation\n",
                "        })\n",
                "        \n",
                "        if (i + 1) % 5 == 0:\n",
                "            print(f\"  Translated {i + 1}/{len(segments)}\")\n",
                "    \n",
                "    return jsonify({\n",
                "        'status': 'success',\n",
                "        'translated_segments': translated_segments\n",
                "    })\n",
                "\n",
                "# Start ngrok tunnel\n",
                "print(\"Starting ngrok tunnel...\")\n",
                "public_url = ngrok.connect(5000)\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"üöÄ Colab GPU Server is running!\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"Public URL: {public_url}\")\n",
                "print(f\"\\nAdd this to your local .env file:\")\n",
                "print(f\"COLAB_API_URL={public_url}\")\n",
                "print(f\"USE_COLAB_GPU=True\")\n",
                "print(f\"{'='*60}\\n\")\n",
                "\n",
                "# Run Flask\n",
                "app.run(port=5000)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## What Happens Next\n",
                "\n",
                "Once this cell is running:\n",
                "\n",
                "1. **You'll see a public URL** (e.g., `https://xxxx.ngrok.io`)\n",
                "2. **Copy that URL** and add it to your local `.env` file\n",
                "3. **Set `USE_COLAB_GPU=True`** in your `.env`\n",
                "4. **Restart your local FastAPI server**\n",
                "5. **Upload videos as normal** - they'll use Colab's GPU automatically!\n",
                "\n",
                "The server will:\n",
                "- Auto-load Whisper model on first use\n",
                "- Auto-load LLM model on first use\n",
                "- Keep models in memory for fast processing\n",
                "- Process requests from your local machine\n",
                "\n",
                "‚ö†Ô∏è Keep this notebook open and the cell running while processing videos!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
