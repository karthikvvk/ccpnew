{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install  flask flask-cors pyngrok openai-whisper transformers accelerate bitsandbytes sentencepiece \n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!ngrok config add-authtoken 35QmdBdgUvzBVxpSZRTs4iAEgu9_2w7NRNiXPG4QeANGqmYHQ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Install dependencies\n",
                "!pip install -q flask flask-cors pyngrok\n",
                "!pip install -q openai-whisper\n",
                "!pip install -q transformers accelerate bitsandbytes sentencepiece\n",
                "!pip install -q edge-tts\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import os\n",
                "import asyncio\n",
                "from flask import Flask, request, jsonify, send_file\n",
                "from flask_cors import CORS\n",
                "import whisper\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
                "from pyngrok import ngrok\n",
                "import tempfile\n",
                "import edge_tts\n",
                "\n",
                "app = Flask(__name__)\n",
                "CORS(app)\n",
                "\n",
                "# Global model cache\n",
                "whisper_model = None\n",
                "refiner_model = None\n",
                "refiner_tokenizer = None\n",
                "\n",
                "# Edge TTS voices by language\n",
                "EDGE_VOICES = {\n",
                "    'en': 'en-US-AriaNeural',\n",
                "    'es': 'es-ES-AlvaroNeural',\n",
                "    'fr': 'fr-FR-DeniseNeural',\n",
                "    'de': 'de-DE-ConradNeural',\n",
                "    'it': 'it-IT-DiegoNeural',\n",
                "    'pt': 'pt-BR-FranciscaNeural',\n",
                "    'ja': 'ja-JP-NanamiNeural',\n",
                "    'ko': 'ko-KR-InJoonNeural',\n",
                "    'zh-cn': 'zh-CN-XiaoxiaoNeural',\n",
                "    'hi': 'hi-IN-MadhurNeural'\n",
                "}\n",
                "\n",
                "@app.route('/health', methods=['GET'])\n",
                "def health():\n",
                "    '''Health check'''\n",
                "    return jsonify({\n",
                "        'status': 'healthy',\n",
                "        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\",\n",
                "        'whisper_loaded': whisper_model is not None,\n",
                "        'refiner_loaded': refiner_model is not None\n",
                "    })\n",
                "\n",
                "# ============ WHISPER ============\n",
                "@app.route('/load_whisper', methods=['POST'])\n",
                "def load_whisper():\n",
                "    '''Load Whisper model on GPU'''\n",
                "    global whisper_model\n",
                "    \n",
                "    data = request.json or {}\n",
                "    model_size = data.get('model_size', 'medium')\n",
                "    \n",
                "    print(f\"Loading Whisper {model_size} on GPU...\")\n",
                "    whisper_model = whisper.load_model(model_size, device=\"cuda\")\n",
                "    print(\"âœ… Whisper loaded!\")\n",
                "    \n",
                "    return jsonify({'status': 'success', 'model': model_size})\n",
                "\n",
                "@app.route('/whisper/transcribe', methods=['POST'])\n",
                "def transcribe():\n",
                "    '''Transcribe audio using Whisper on GPU'''\n",
                "    global whisper_model\n",
                "    \n",
                "    if whisper_model is None:\n",
                "        return jsonify({'error': 'Call /load_whisper first'}), 400\n",
                "    \n",
                "    if 'audio' not in request.files:\n",
                "        return jsonify({'error': 'No audio file'}), 400\n",
                "    \n",
                "    audio_file = request.files['audio']\n",
                "    language = request.form.get('language', None)\n",
                "    \n",
                "    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as f:\n",
                "        audio_file.save(f.name)\n",
                "        temp_path = f.name\n",
                "    \n",
                "    try:\n",
                "        print(\"Transcribing...\")\n",
                "        result = whisper_model.transcribe(\n",
                "            temp_path,\n",
                "            language=language if language != 'auto' else None,\n",
                "            verbose=False\n",
                "        )\n",
                "        os.unlink(temp_path)\n",
                "        return jsonify({'status': 'success', 'result': result})\n",
                "    except Exception as e:\n",
                "        os.unlink(temp_path)\n",
                "        return jsonify({'error': str(e)}), 500\n",
                "\n",
                "# ============ LLM REFINER ============\n",
                "@app.route('/load_refiner', methods=['POST'])\n",
                "def load_refiner():\n",
                "    '''Load LLM refiner (Flan-T5) on GPU'''\n",
                "    global refiner_model, refiner_tokenizer\n",
                "    \n",
                "    data = request.json or {}\n",
                "    model_name = data.get('model_name', 'google/flan-t5-large')\n",
                "    \n",
                "    print(f\"Loading {model_name} on GPU...\")\n",
                "    refiner_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    \n",
                "    if 't5' in model_name.lower() or 'flan' in model_name.lower():\n",
                "        refiner_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
                "            model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
                "        )\n",
                "    else:\n",
                "        refiner_model = AutoModelForCausalLM.from_pretrained(\n",
                "            model_name, torch_dtype=torch.float16, device_map=\"auto\", load_in_8bit=True\n",
                "        )\n",
                "    print(\"âœ… Refiner loaded!\")\n",
                "    \n",
                "    return jsonify({'status': 'success', 'model': model_name})\n",
                "\n",
                "@app.route('/load_llm', methods=['POST'])\n",
                "def load_llm():\n",
                "    '''Alias for load_refiner'''\n",
                "    return load_refiner()\n",
                "\n",
                "@app.route('/llm/refine', methods=['POST'])\n",
                "def refine():\n",
                "    '''Refine/fix transcription segments using LLM on GPU'''\n",
                "    global refiner_model, refiner_tokenizer\n",
                "    \n",
                "    if refiner_model is None:\n",
                "        return jsonify({'error': 'Call /load_refiner first'}), 400\n",
                "    \n",
                "    data = request.json\n",
                "    segments = data.get('segments', [])\n",
                "    visual_context = data.get('visual_context', None)\n",
                "    \n",
                "    print(f\"Refining {len(segments)} segments on GPU...\")\n",
                "    refined_segments = []\n",
                "    \n",
                "    for i, seg in enumerate(segments):\n",
                "        text = seg.get('text', '')\n",
                "        \n",
                "        if visual_context:\n",
                "            prompt = f\"Fix grammar and complete sentences. Context: {visual_context}. Text: {text}\"\n",
                "        else:\n",
                "            prompt = f\"Fix grammar and complete sentences: {text}\"\n",
                "        \n",
                "        inputs = refiner_tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True).to(\"cuda\")\n",
                "        with torch.no_grad():\n",
                "            outputs = refiner_model.generate(**inputs, max_new_tokens=256, temperature=0.3, do_sample=True)\n",
                "        \n",
                "        refined = refiner_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "        \n",
                "        refined_segments.append({\n",
                "            'start': seg['start'],\n",
                "            'end': seg['end'],\n",
                "            'original': text,\n",
                "            'refined': refined.strip()\n",
                "        })\n",
                "        \n",
                "        if (i + 1) % 10 == 0:\n",
                "            print(f\"  {i + 1}/{len(segments)} done\")\n",
                "    \n",
                "    print(\"âœ… Refinement complete!\")\n",
                "    return jsonify({'status': 'success', 'refined_segments': refined_segments})\n",
                "\n",
                "# ============ TTS (edge-tts) ============\n",
                "@app.route('/tts/generate', methods=['POST'])\n",
                "def generate_tts():\n",
                "    '''Generate speech using edge-tts'''\n",
                "    text = request.form.get('text', '')\n",
                "    language = request.form.get('language', 'en')\n",
                "    \n",
                "    if not text:\n",
                "        return jsonify({'error': 'No text provided'}), 400\n",
                "    \n",
                "    voice = EDGE_VOICES.get(language, 'en-US-AriaNeural')\n",
                "    \n",
                "    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as f:\n",
                "        output_path = f.name\n",
                "    \n",
                "    async def generate():\n",
                "        communicate = edge_tts.Communicate(text, voice)\n",
                "        await communicate.save(output_path)\n",
                "    \n",
                "    try:\n",
                "        print(f\"Generating TTS: {text[:50]}...\")\n",
                "        asyncio.run(generate())\n",
                "        return send_file(output_path, mimetype='audio/mpeg')\n",
                "    except Exception as e:\n",
                "        return jsonify({'error': str(e)}), 500\n",
                "\n",
                "# Start server\n",
                "print(\"Starting ngrok tunnel...\")\n",
                "public_url = ngrok.connect(5000)\n",
                "print(f\"\\\\n{'='*50}\")\n",
                "print(f\"ðŸš€ COLAB GPU SERVER READY\")\n",
                "print(f\"{'='*50}\")\n",
                "print(f\"URL: {public_url}\")\n",
                "print(f\"\\\\nSet in your local .env:\")\n",
                "print(f\"  COLAB_API_URL={public_url}\")\n",
                "print(f\"  USE_COLAB_GPU=True\")\n",
                "print(f\"{'='*50}\")\n",
                "print(f\"\\\\nEndpoints:\")\n",
                "print(f\"  POST /load_whisper       - Load Whisper\")\n",
                "print(f\"  POST /load_refiner       - Load Flan-T5\")\n",
                "print(f\"  POST /whisper/transcribe - Transcribe audio\")\n",
                "print(f\"  POST /llm/refine         - Fix sentences\")\n",
                "print(f\"  POST /tts/generate       - Generate speech\")\n",
                "print(f\"{'='*50}\\\\n\")\n",
                "\n",
                "app.run(port=5000)\n",
                "# \"\"\"\n",
                "\n",
                "# print(\"=\"*60)\n",
                "# print(\"COLAB GPU SERVER - Whisper + LLM + Edge-TTS\")\n",
                "# print(\"=\"*60)\n",
                "# print(\"\\\\nThis server handles:\")\n",
                "# print(\"  1. Whisper transcription (GPU)\")\n",
                "# print(\"  2. LLM sentence refinement (GPU)\")\n",
                "# print(\"  3. Edge-TTS speech synthesis (stable, high quality)\")\n",
                "# print(\"\\\\nTranslation, video processing run locally.\")\n",
                "# print(\"=\"*60)\n",
                "# print(\"\\\\nCopy below into a Colab cell:\")\n",
                "# print(\"=\"*60)\n",
                "# print(CODE)\n"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
